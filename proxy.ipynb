{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "## Background\n",
    "\n",
    "`proxy.py` was released on 20th August, 2013 as a single file HTTP proxy server implementation with no external dependencies.  See the [first commit](https://github.com/abhinavsingh/proxy.py/commit/75044a72d9c7b4b8910ba551006b801eafdf3c47) and [read introductory blog](https://abhinavsingh.com/proxy-py-a-lightweight-single-file-http-proxy-server-in-python/) to get an insight about why `proxy.py` was created.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Today, `proxy.py` has matured into a full blown networking library with focus on being lightweight, ability to deliver maximum performance while being extendible.  Unlike other Python servers, `proxy.py` doesn't need a `WSGI` or `UWSI` frontend, which then usually has to be placed behind a reverse proxy e.g. `Nginx` or `Apache`.  Of-course, `proxy.py` can be placed directly behind a load-balancer _(optionally capable of speaking HA proxy protocol)_.\n",
    "\n",
    "## The Concept Of Work\n",
    "\n",
    "`proxy.py` core is written with a high level concept of `work`.\n",
    "\n",
    "- A running instance can receive `work` from one or multiple `sources`\n",
    "  - Example, when `proxy.py` starts, an accepted client connection is a `work` coming from TCP socket `sources`\n",
    "- Handlers can be written to process various types of `work`\n",
    "  - Example, `HttpProtocolHandler` handles HTTP client connections `work`\n",
    "- A client connection can come from a variety of `sources`\n",
    "  - TCP sockets\n",
    "  - UDP sockets\n",
    "  - Unix sockets\n",
    "  - Raw sockets\n",
    "\n",
    "Infact, `work` can be any processing unit.  It doesn't have to be a client connection.  Example:\n",
    "\n",
    "- A file on disk can act as the `source` and each line in that file as the `work` definition\n",
    "- Imagine tailing a file on disk as `source` and processing each line as a separate `work` object\n",
    "- If you want, each line in the file can also be a URL to be scrapped or download\n",
    "- If you want, your `work` handlers can append new URLs _(discovered by scrapping previous URL entries)_ back in the file, creating an infinite feedback loop between the `work` processing core.\n",
    "\n",
    "And just like that we have created a web scraper!!!\n",
    "\n",
    "To extend this generic concept, now imagine a distributed queue as the `source` of our `work`, where each published message in the queue is our `work` payload.  Some examples of such `sources` can be:\n",
    "- A `Redis` channel\n",
    "- Google Cloud PubSub channel\n",
    "- Kafka queues\n",
    "\n",
    "And just like that we have created a distributed `work` executor!!!\n",
    "\n",
    "## HttpParser\n",
    "\n",
    "`HttpParser` class is at the heart of everything related to HTTP.  It is used by Web server and Proxy server core and their plugin eco-system.  As the name suggests, it is capable of parsing both HTTP request and response packets.  It can also parse HTTP look-a-like protocols like ICAP, SIP etc.  Most importantly, remember that `HttpParser` was originally written to handle HTTP packets arriving in the context of a proxy server and till date its default behavior favors the same flavor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's start by parsing a HTTP web request using `HttpParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "from proxy.http.methods import httpMethods\n",
    "from proxy.http.parser import HttpParser, httpParserTypes, httpParserStates\n",
    "from proxy.common.constants import HTTP_1_1\n",
    "\n",
    "request = HttpParser(httpParserTypes.REQUEST_PARSER)\n",
    "request.parse(b'GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n')\n",
    "\n",
    "assert request.state == httpParserStates.COMPLETE\n",
    "assert request.method == httpMethods.GET\n",
    "assert request.version == HTTP_1_1\n",
    "assert request.host == None\n",
    "assert request.port == 80\n",
    "assert request._url != None\n",
    "assert request._url.remainder == b'/'\n",
    "assert request.has_header(b'host')\n",
    "assert request.header(b'host') == b'localhost'\n",
    "assert len(request.headers) == 1\n",
    "\n",
    "print(request.build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, let's parse a HTTP proxy request using `HttpParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'GET /get HTTP/1.1\\r\\nHost: httpbin.org\\r\\n\\r\\n'\n",
      "b'GET http://httpbin.org:80/get HTTP/1.1\\r\\nHost: httpbin.org\\r\\n\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "request = HttpParser(httpParserTypes.REQUEST_PARSER)\n",
    "request.parse(b'GET http://httpbin.org/get HTTP/1.1\\r\\nHost: httpbin.org\\r\\n\\r\\n')\n",
    "\n",
    "assert request.state == httpParserStates.COMPLETE\n",
    "assert request.method == httpMethods.GET\n",
    "assert request.version == HTTP_1_1\n",
    "assert request.host == b'httpbin.org'\n",
    "assert request.port == 80\n",
    "assert request._url != None\n",
    "assert request._url.remainder == b'/get'\n",
    "assert request.has_header(b'host')\n",
    "assert request.header(b'host') == b'httpbin.org'\n",
    "assert len(request.headers) == 1\n",
    "\n",
    "print(request.build())\n",
    "print(request.build(for_proxy=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `request.build()` and `request.build(for_proxy=True)` behave for a HTTP proxy request.  Also, notice how `request.host` field was populated for HTTP proxy request but not for the prior HTTP web request example.\n",
    "\n",
    "> To conclude, let's parse a HTTPS proxy request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'CONNECT / HTTP/1.1\\r\\nHost: httpbin.org:443\\r\\n\\r\\n'\n",
      "b'CONNECT httpbin.org:443 HTTP/1.1\\r\\nHost: httpbin.org:443\\r\\n\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "request = HttpParser(httpParserTypes.REQUEST_PARSER)\n",
    "request.parse(b'CONNECT httpbin.org:443 HTTP/1.1\\r\\nHost: httpbin.org:443\\r\\n\\r\\n')\n",
    "\n",
    "assert request.state == httpParserStates.COMPLETE\n",
    "assert request.method == httpMethods.CONNECT\n",
    "assert request.version == HTTP_1_1\n",
    "assert request.host == b'httpbin.org'\n",
    "assert request.port == 443\n",
    "assert request._url != None\n",
    "assert request._url.remainder == None\n",
    "assert request.has_header(b'host')\n",
    "assert request.header(b'host') == b'httpbin.org:443'\n",
    "assert len(request.headers) == 1\n",
    "\n",
    "print(request.build())\n",
    "print(request.build(for_proxy=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da9d6927d62b2b95bde149eedfbd5367cb7f465aad65a736f49c99ee3db39df7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('venv310': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
